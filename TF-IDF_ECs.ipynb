{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/salisburyfamily/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/salisburyfamily/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/salisburyfamily/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "import time, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc\n",
    "# tensorflow is needed as a dependency for something else\n",
    "\n",
    "'''\n",
    "To run the following code, you can run the following 3 lines:\n",
    "\n",
    "from model import review_invoices\n",
    "review_invoices = review_invoices()\n",
    "review_invoices.run()\n",
    "'''\n",
    "\n",
    "class review_invoices:\n",
    "    '''\n",
    "    I put all of David's code into a class. I split everything within the class\n",
    "    into methods. I didn't adjust much of the code. Pretty much all I did was \n",
    "    combine the two csv's into one dataframe since the csv's had to be split\n",
    "    into two.\n",
    "    \n",
    "    There is opportunity to adjust and restructure the methods within this class\n",
    "    in a way that makes more sense. I just want to get a good framework for the\n",
    "    code before we start expanding on it.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize variables. Everything within this __init__ method gets\n",
    "        run automatically when the review_invoices class gets called. This is\n",
    "        a great place to define variables and possibly run methods automatically\n",
    "        \n",
    "        Every variable defined within this method can be called and viewed by \n",
    "        the user. Conversely, anything within the below methods is private.\n",
    "        \n",
    "        To make a variable callable/called by other methods, put \n",
    "        'self.' in front of the variable. This brings the variable outside of \n",
    "        the method and into the class.\n",
    "        '''\n",
    "        print('Initializing')\n",
    "        # Load the data into dataframes\n",
    "        self.df_1 = pd.read_csv('Data/data.csv')\n",
    "        self.df_2 = pd.read_csv('Data/data2.csv')\n",
    "        df = self.df_1.append(self.df_2, ignore_index = True)\n",
    "        # Rename column headers\n",
    "        df.rename(columns = {'WO #':'work_order_id', 'Chargeback':'liability', \n",
    "            'Terms':'work_order'}, inplace = True)\n",
    "        self.df = df\n",
    "        # Update Pandas settings. View full contents of each column\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        # Display up to 10 columns\n",
    "        pd.set_option('display.max_columns', 10)\n",
    "        # A check for null values\n",
    "        self.null = df.isnull().values.any()\n",
    "\n",
    "    def explore_data(self):\n",
    "        print('Running explore_data()')\n",
    "        # Define the raw dataframe\n",
    "        df = self.df\n",
    "        # Print basic info about dataframe\n",
    "        print('\\nOriginal dataframe info')\n",
    "        print('----------------------------------------')\n",
    "        df.info()\n",
    "        print('----------------------------------------')\n",
    "        # Print out first 5 rows of the df\n",
    "        print(f'\\nAre there any null values? {self.null}')\n",
    "        print('\\nPrinting the first 5 rows of the original dataframe')\n",
    "        display(df.head())\n",
    "        # Create csv of duplicate terms to be audited\n",
    "        duplicate_terms = df[df.duplicated(subset=['work_order'], keep = False)]\n",
    "        self.duplicate_terms = duplicate_terms.sort_values(by=['work_order'])\n",
    "        duplicate_terms['work_order_id'].nunique()\n",
    "        # Create csv of duplicate work order numbers to be audited\n",
    "        duplicate_wo = df[df.duplicated(subset=['work_order_id'], keep = False)]\n",
    "        self.duplicate_wo = duplicate_wo.sort_values(by=['work_order_id'])\n",
    "\n",
    "    def clean_df(self):\n",
    "        print('Running clean_df()')\n",
    "        df = self.df\n",
    "        # Remove any rows with a null cell\n",
    "        if self.null is True:\n",
    "            df = df.dropna()\n",
    "        # Remove rows with invalid terms\n",
    "        print('\\nDropping work orders with invalid text: \"#NAME?\"')\n",
    "        df = df.drop(df[df['work_order'] == '#NAME?'].index)\n",
    "        # Parse out phone numbers into a new column, phone_num\n",
    "        print('Extracting and removing phone numbers')\n",
    "        df['phone_num'] = df['work_order'].str.extract(\n",
    "            '(\\(?\\d\\d\\d\\)?-? ?\\.?\\d\\d\\d-?\\.? ?\\d\\d\\d\\d?)')\n",
    "        # Remove the phone numbers from the work_order column\n",
    "        df['work_order'] = df['work_order'].replace(\n",
    "            '(\\(?\\d\\d\\d\\)?-? ?\\.?\\d\\d\\d-?\\.? ?\\d\\d\\d\\d?)', '', regex = True)\n",
    "        print('Extracting and removing email addresses')\n",
    "        # Extract email addresses and put into separate column\n",
    "        df['email'] = df['work_order'].str.extract('(\\S+@\\S+)')\n",
    "        # Remove email addresses from work_order column\n",
    "        df['work_order'] = df['work_order'].replace('(\\S+@\\S+)', '', regex = True)\n",
    "        print('Removing some meaningless words from work order templates')\n",
    "        # Remove \"Contact:\", \"Email:\", \"Phone:\" from each work order\n",
    "        df['work_order'] = df['work_order'].replace('(Contact:|Email:|Phone:)', \n",
    "            '', regex=True)\n",
    "        print('Extracting and removing property ID\\'s')\n",
    "        # Extract the property ID from the end of each work order\n",
    "        df['property_id'] = df['work_order'].str.rsplit(' ', 1).str[1]\n",
    "        # Remove the property ID from each work order\n",
    "        df['work_order'] = df['work_order'].str.rsplit(' ', 1).str[0]\n",
    "        # Replace any non-word characters from work_order column with a space\n",
    "        print('Replacing all non-word characters with a space')\n",
    "        df['work_order'] = df['work_order'].str.replace('\\W', ' ', regex = True)\n",
    "        # Make the work_order column all lower case\n",
    "        print('Making work_order column all lower case')\n",
    "        df['work_order'] = df['work_order'].str.lower()\n",
    "        print('Turning column of strings into column of lists (This takes some '\n",
    "            'time)')\n",
    "        df['work_order'] = df['work_order'].apply(word_tokenize)\n",
    "        # Make clean dataframe callable outside of the method\n",
    "        # The index was messed up after removing some rows, need to reset_index\n",
    "        df = df.reset_index(drop = True)\n",
    "        \n",
    "        self.df_clean = df\n",
    "        # Review some of the changes made to the data\n",
    "        df_clean = df\n",
    "        print('\\nCleaned dataframe info')\n",
    "        print('----------------------------------------')\n",
    "        df_clean.info()\n",
    "        print('----------------------------------------')\n",
    "        print('\\nPrinting the first 5 rows of the clean dataframe')\n",
    "        display(df_clean.head())\n",
    "        # Convert dataframe columns to series for later method use\n",
    "        self.X = df[\"work_order\"]\n",
    "        self.y = df[\"liability\"]\n",
    "        return df_clean\n",
    "        \n",
    "\n",
    "    def link_words(self):\n",
    "        # Further clean and then lemmatize\n",
    "        print('Running link_words()')\n",
    "        # Define the work_order column as X\n",
    "        X = self.X\n",
    "        # Create an empty list called documents used to append lemmatized text\n",
    "        documents = []\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        print('\\nLemmatizing. This one takes some time too...')\n",
    "        # Lemmatize each word from each list of words, one at at time\n",
    "        # Join those words together into strings, like they started\n",
    "        # Append each string onto the documents list\n",
    "        for sen in range(0, len(X)):\n",
    "            document = X[sen]\n",
    "            document = [stemmer.lemmatize(word) for word in document]\n",
    "            document = ' '.join(document)\n",
    "            documents.append(document)\n",
    "        \n",
    "        print('Creating equivalence classes...')\n",
    "        def create_ec(dictionary, corpus):\n",
    "            for key, values in dictionary.items():\n",
    "                for value in values:\n",
    "                    corpus= [item.replace(value, key) for item in corpus]\n",
    "            return corpus\n",
    "\n",
    "        corpus = documents\n",
    "        res_dic = {'resident': ['tenant', 'renter', 'occupant']}\n",
    "        corpus = create_ec(res_dic, corpus)\n",
    "        landlord_dic = {'landlord': ['owner','manager']}\n",
    "        corpus = create_ec(landlord_dic, corpus)\n",
    "        tech_dic = {'technician': ['tech']}\n",
    "        corpus = create_ec(tech_dic, corpus)\n",
    "        house_dic = {'house': ['home','property']}\n",
    "        corpus = create_ec(house_dic, corpus)\n",
    "        fridge_dic = {'refrigerator': ['fridge']}\n",
    "        corpus = create_ec(fridge_dic, corpus)\n",
    "        air_dic= {'air': ['ac', 'air conditioning']}\n",
    "        corpus = create_ec(air_dic, corpus)\n",
    "        bath_dic = {'bath': ['tub', 'bathtub']}\n",
    "        corpus = create_ec(bath_dic, corpus)\n",
    "        heater_dic= {'heater': ['furnace']}\n",
    "        corpus = create_ec(heater_dic, corpus)\n",
    "        temp_dic= {'temperature': ['temp']}\n",
    "        corpus = create_ec(temp_dic, corpus)\n",
    "        roof_dic = {'roof': ['roofing', 'shingles', 'shingle']}\n",
    "        corpus = create_ec(roof_dic, corpus)\n",
    "        documents = corpus\n",
    "        \n",
    "        print('Dropping words with less than 3 letters...')\n",
    "        newdocuments =[]\n",
    "        for row in documents:\n",
    "            shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "            row1 = (shortword.sub('',row))\n",
    "            newdocuments.append(row1)\n",
    "        documents = newdocuments\n",
    "        \n",
    "        self.documents = documents\n",
    "        # Print out first five items in documents list\n",
    "        print('\\nWe\\'ve turned the work_order column into a list called '\n",
    "            '\"documents\"')\n",
    "\n",
    "\n",
    "        \n",
    "    def vectorize(self):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "        tfidfconverter = TfidfVectorizer(\n",
    "            max_features=2000,\n",
    "            min_df=10,\n",
    "            max_df=0.7,\n",
    "            stop_words=stopwords.words('english'))  \n",
    "        self.X = tfidfconverter.fit_transform(self.documents).toarray()  \n",
    "        \n",
    "        tfidf_result = tfidfconverter.fit_transform(self.documents)\n",
    "        \n",
    "        scores = zip(tfidfconverter.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print('\\n Printing the top 20 TFIDF scores...\\n')\n",
    "        for item in sorted_scores[0:20]:\n",
    "            print (\"{0:50} Score: {1}\".format(item[0], item[1]))\n",
    "\n",
    "    def partition(self):\n",
    "        from sklearn.model_selection import train_test_split  \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, \n",
    "            self.y, test_size=.2, random_state=1)  \n",
    "\n",
    "    def model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2000, input_dim = 2000, activation = 'relu'))\n",
    "        model.add(Dense(1000, activation = 'relu'))\n",
    "        model.add(Dense(500, activation = 'relu'))\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "        model.compile(optimizer = 'adam', loss = 'binary_crossentropy', \n",
    "            metrics = ['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        This method can be called as an easy way to run all of the above methods\n",
    "        and the commands to get output. \n",
    "        \n",
    "        The other easy alternative is to include all of this stuff in the\n",
    "        __init__ method so it get's run automatically when the class is called.\n",
    "        Splitting all of this into its own method just makes the class easier to\n",
    "        control\n",
    "        '''\n",
    "        self.clean_df()\n",
    "        self.link_words()\n",
    "        self.vectorize()\n",
    "        self.partition()\n",
    "        self.model()\n",
    "        \n",
    "        # -------- call model -------- \n",
    "        model = self.model()\n",
    "        \n",
    "        # -------- fit  -------- \n",
    "        model.fit(self.X_train,self.y_train, epochs = 10, batch_size = 512, \n",
    "            verbose = True)\n",
    "        \n",
    "        # -------- predict  -------- \n",
    "        pred = model.predict_classes(self.X_test)\n",
    "        \n",
    "        # -------- Confusion Matrix -------- \n",
    "        matrix = pd.DataFrame(confusion_matrix(self.y_test,pred, \n",
    "            labels = [x for x in range(0,2)]))\n",
    "        matrix\n",
    "        \n",
    "        # -------- accuracy -------- \n",
    "        balanced_accuracy_score(self.y_test,pred)\n",
    "        \n",
    "        # -------- summary -------- \n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri = review_invoices()\n",
    "ri.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
